{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import io\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "\n",
    "import d2lzh as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics(f_name):\n",
    "    #read lyrics (Japanese)\n",
    "    '''\n",
    "    f_name: preprocessed lyric txt file\n",
    "    corpus_indices: list of character index\n",
    "    char_to_idx: dictionary\n",
    "    idx_to_char: dictionary\n",
    "    vocab_size: int, the number of distinct character\n",
    "    '''\n",
    "    el_file = open(f_name + \".txt\",'r', encoding = 'gbk')\n",
    "    text = el_file.read().lower()\n",
    "    corpus_chars = text.replace('\\n', ' ')\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_indices, char_to_idx, idx_to_char, vocab_size = load_lyrics('hoshino2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 5, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(X, size):\n",
    "    return [nd.one_hot(x, size) for x in X.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = nd.zeros(num_hiddens, ctx=ctx)\n",
    "\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens, ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, ctx):\n",
    "    norm = nd.array([0], ctx)\n",
    "    for param in params:\n",
    "        norm += (param.grad ** 2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size, ctx, corpus, idx_to_char, char_to_idx, is_random_iter, num_epochs,\n",
    "                          num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:\n",
    "            state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:\n",
    "                state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            with autograd.record():\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                (outputs, state) = rnn(inputs, state, params)\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                y = Y.T.reshape((-1,))\n",
    "                l = loss(outputs, y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params, clipping_theta, ctx)\n",
    "            d2l.sgd(params, lr, 1)\n",
    "            l_sum += l.asscalar() * y.size\n",
    "            n += y.size\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d,perplexity %f,time %.2f sec' %\n",
    "                  (epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(prefix, pred_len, rnn, params,\n",
    "                                        init_rnn_state, num_hiddens, vocab_size, ctx,\n",
    "                                        idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nd.arange(10).reshape((2,5))\n",
    "inputs = to_onehot(X, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_rnn_state(X.shape[0], num_hiddens, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = to_onehot(X.as_in_context(ctx), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, state_new = rnn(inputs, state, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (2, 1068) (2, 256)\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs), outputs[0].shape, state_new[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "??d2l.data_iter_consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "笑顔使熟ワ雪溢腹銀口儚覚\n"
     ]
    }
   ],
   "source": [
    "print(predict_rnn('笑顔', 10, rnn, params, init_rnn_state, num_hiddens,\n",
    "                  vocab_size, ctx, idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 400, 35, 32, 1e2, 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_period, pred_len, prefixes = 50, 50, ['笑顔']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50,perplexity 14.293405,time 5.98 sec\n",
      " - 笑顔ではいだ あの娘を染めて広がる 動けない場所からいつかない まだ何ものいだ あの娘を くれたち来て \n",
      "epoch 100,perplexity 4.853905,time 6.47 sec\n",
      " - 笑顔のよう 君が覚の海 愛を歌ぶ 君の中を染めて広がる ただ地獄を進む者が悲しい記憶に勝つ 作り物だ世界\n",
      "epoch 150,perplexity 3.123811,time 6.26 sec\n",
      " - 笑顔では 次のあなた ここで会いだから楽しい 誰かにないなるんで いる もんだならしいな僕らし するだら\n",
      "epoch 200,perplexity 2.432333,time 6.15 sec\n",
      " - 笑顔では 次のあなたへの 橋になりますように 遠い場所も繋がっているよ 外の話を聞くさんならう 生まる胸\n",
      "epoch 250,perplexity 2.061410,time 6.34 sec\n",
      " - 笑顔では どんな時ものれなには 心のあふれぬまで 輝くしゃもの水さらぬ 病だあるうべり捨いだけ 愛をはじ\n",
      "epoch 300,perplexity 1.815772,time 6.21 sec\n",
      " - 笑顔では いつもの他倍を 歌アス濡るここのこのだら どうかの中でしいえも ああながらいつものう 何かには\n",
      "epoch 350,perplexity 1.671480,time 6.11 sec\n",
      " - 笑顔ではいだなささ もんはさえないけど いつもでもないて イ跡われたまま 世界鳴らす言うな顔が うだろう\n",
      "epoch 400,perplexity 1.567706,time 5.98 sec\n",
      " - 笑顔できただな 空に変わるけべて 光が辿う すかいなには笑うなさ もの丈ない指のこにあるものいつもの家倍\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                      vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,\n",
    "                      clipping_theta, batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('みんなが瞳にあるから したもの空を 日々のわみためは 次の口かを照らすんだ 命は続く日々のゲームは続く 君')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
